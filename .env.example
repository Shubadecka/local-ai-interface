# Docker Compose ports
OLLAMA_PORT=11434
COMFYUI_PORT=8188
OPENWEBUI_PORT=3000

# Ollama connection for finetuning CLI
OLLAMA_HOST=http://localhost:11434

# HuggingFace token (optional, for gated models)
# HF_TOKEN=your_token_here

# Weights & Biases (optional, for experiment tracking)
# WANDB_API_KEY=your_key_here
# WANDB_PROJECT=llm-finetune

# Number of threads for pytorch and numpy
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4

# Disable torch.compile to work with ROCm
TORCH_COMPILE_DISABLE=1
